# -*- coding: utf-8 -*-
"""MNIST_Assignment_1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BsOMK2g6vQx1LblBwxIss9ksl-lsx_GQ

### Get the MNIST Data
"""

#!pip install tensorflow-gpu==2.0.0-alpha0

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import tensorflow as tf
print(tf.__version__)
print(tf.test.gpu_device_name())
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

from google.colab import files
src = list(files.upload().values())[0]
open('input_data.py','wb').write(src)
# source :
# https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/input_data.py
# saved the file on local host and then uploaded

import input_data

mnist = input_data.read_data_sets("MNIST_data/",one_hot=True)

type(mnist)

mnist.train.images

mnist.train.num_examples

mnist.test.num_examples

mnist.validation.num_examples

"""### Visualizing the Data"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
# %matplotlib inline

mnist.train.images[1].shape

x_example,y_example = mnist.train.next_batch(100)
plt.imshow(x_example[1].reshape(28,28))
print(y_example[1])

x_example.reshape(100,28,28).shape

plt.imshow(mnist.train.images[1].reshape(28,28),cmap='gist_gray')

"""## Create the Model

Helper Functions
"""

def init_weights(shape):
    init_random_dist = tf.truncated_normal(shape, stddev=0.1)
    return tf.Variable(init_random_dist)

def init_bias(shape):
    init_bias_vals = tf.constant(0.1, shape=shape)
    return tf.Variable(init_bias_vals)

def conv2d(x, W):
    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')

#def conv2d_stride(x,W):
#    return tf.nn.conv2d(x,W, strides=[1,2,2,1], padding='SAME')

def max_pool_2by2(x):
    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],
                          strides=[1, 2, 2, 1], padding='SAME')

def convolutional_layer(input_x, shape):
    W = init_weights(shape)
    b = init_bias([shape[3]])
    return tf.nn.relu(conv2d(input_x, W) + b)

def convolutional_layer_padding(input_x, shape):
    W = init_weights(shape)
    b = init_bias([shape[3]])
    return tf.nn.relu(conv2d_stride(input_x, W) + b)


def normal_full_layer(input_layer, size):
    input_size = int(input_layer.get_shape()[1])
    W = init_weights([input_size, size])
    b = init_bias([size])
    return tf.matmul(input_layer, W) + b

"""Placeholders"""

x = tf.placeholder(tf.float32,shape=[None,784])
y_true = tf.placeholder(tf.float32,shape=[None,10])

# for dropout
hold_prob = tf.placeholder(tf.float32)

# for batch_normalization
#is_train = tf.placeholder(tf.bool, name="is_train")

"""Layers"""

x_image = tf.reshape(x,[-1,28,28,1])

convo_1_1 = convolutional_layer(x_image, shape=[3,3,1,32])
convo_1_2 = convolutional_layer(convo_1_1, shape=[3,3,32,32])
#convo_1_3 = convolutional_layer_padding(convo_1_2, shape=[5,5,32,32])
convo_1_pooling = max_pool_2by2(convo_1_2)
#convo_normal_1 = tf.layers.batch_normalization(convo_1_pooling, training=is_train)
convo_1_dropout = tf.nn.dropout(convo_1_pooling, keep_prob=hold_prob)

convo_2_1 = convolutional_layer(convo_1_dropout, shape=[3,3,32,64])
convo_2_2 = convolutional_layer(convo_2_1, shape=[3,3,64,64])
#convo_2_3 = convolutional_layer_padding(convo_2_2, shape=[5,5,64,64])
convo_2_pooling = max_pool_2by2(convo_2_2)
#convo_normal_2 = tf.layers.batch_normalization(convo_2_pooling, training=is_train)
convo_2_dropout = tf.nn.dropout(convo_2_pooling, keep_prob=hold_prob)

convo_2_flat = tf.reshape(convo_2_dropout,[-1,7*7*64])
# using n_out = [(n_in+2p-k)/s]+1

full_layer_one = tf.nn.relu(normal_full_layer(convo_2_flat,128))
#full_layer_one_normal = tf.layers.batch_normalization(full_layer_one,training=is_train)
full_one_dropout = tf.nn.dropout(full_layer_one,keep_prob=hold_prob)

y_pred = normal_full_layer(full_one_dropout,10)

"""Loss and Optimizer"""

# Loss Function
cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_true,logits=y_pred))

# Optimizer
optimizer = tf.train.AdamOptimizer(learning_rate=0.001)
train = optimizer.minimize(cross_entropy)

# Accuracy
matches = tf.equal(tf.argmax(y_pred,1),tf.argmax(y_true,1))
accuracy = tf.reduce_mean(tf.cast(matches,tf.float32))

"""### Create Session"""

init = tf.global_variables_initializer()

steps = 10000

with tf.Session() as sess:
  
  sess.run(init)

  train_loss = []
  test_loss = []
  train_accuracy = []
  test_accuracy = []
  summary_writer = tf.summary.FileWriter('./Output', sess.graph)

  for i in range(steps):
    batch_x, batch_y = mnist.train.next_batch(50)

    opt = sess.run(train,feed_dict={x:batch_x,y_true:batch_y,hold_prob:0.6})#,is_train:True})

    loss, acc = sess.run([cross_entropy,accuracy],feed_dict={x:batch_x,y_true:batch_y,hold_prob:1.0})#,is_train:False})

    val_loss,test_acc = sess.run([cross_entropy,accuracy],feed_dict={x:mnist.test.images,y_true:mnist.test.labels,hold_prob:1.0})#,is_train:False})

    

    if i%100 == 0:

      print("ON STEP: {}".format(i))

      print("LOSS:")
      print(loss)

      print("TRAINING ACCURACY:")
      print(acc)
      
      print("TESTING ACCURACY:")
      print(test_acc)
      print('\n')

      if i>= 500:

        train_loss.append(loss)
        test_loss.append(val_loss)
        train_accuracy.append(acc)
        test_accuracy.append(test_acc)

plt.plot(range(len(train_loss)), train_loss, 'b', label='Training loss')
plt.plot(range(len(train_loss)), test_loss, 'r', label='Test loss')
plt.title('Training and Test loss')
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Loss',fontsize=16)
plt.legend()
plt.figure()
plt.show()

plt.plot(range(len(train_loss)), train_accuracy, 'b', label='Training Accuracy')
plt.plot(range(len(train_loss)), test_accuracy, 'r', label='Test Accuracy')
plt.title('Training and Test Accuracy')
plt.xlabel('Epochs ',fontsize=16)
plt.ylabel('Loss',fontsize=16)
plt.legend()
plt.figure()
plt.show()

